### 任务赛视觉方案

| 执行模块 | 硬件 | 软件/算法技术栈 | 视觉组核心策略 & 逻辑 (针对单点TOF优化) |
| :---: | :---: | --- | --- |
| **算题识别** | **IMX577**（云台） | **PaddleOCR** (或 Tesseract) | 1. 裁剪画面只保留题目区域ROI<br>2. 识别算式<br>3. 计算<br>4. 发布话题告诉决策层
| **建图** | **Unitree L2** 扫描 | **FAST-LIO2** (或 Point-LIO) | **全局建图与定位**：比赛开始，雷达扫描 6m x 4m 场地 。匹配预设的静态地图，确定机器人在场地的绝对坐标 。 |
| **全局搜索物资箱** | **IMX577** + **L2 雷达**  | **YOLOv8n** (TensorRT加速？) + **Point Cloud Clustering** | **YOLO 识别**箱子类型，将图像中的像素坐标与雷达点云匹配，获取箱子在场地的三维坐标
| **接近目标** | **Unitree L2 实时避障** | **Navigation 2** | **路径规划**：以箱子坐标为目标点，Nav2 自动规划路径 |
| **精准定位** | 机器人停止移动，微调角度，使 **TOF 光斑** 绝对打在箱子上。 | **数据融合** (Kalman Filter 简单版) | **关键解决：**<br>因为 TOF 只能测一点，必须保证 **YOLO框的中心点 ≈ 画面中心点**。<br>当 `abs(dx) < 阈值` 时，读取 TOF 距离数据，发送给机械臂：“前方 35.5cm 处有箱子，准备抓取”。 |  |
| **抓取物资** | **IMX577** 对齐中心 + **TOF** 测距。**机械臂**伸出 | **OpenCV** (Canny边缘检测/颜色分割) | **抓取闭环：** 摄像头微调底盘，使箱子处于正中心。**TOF** 检测机械臂与箱子的实时距离 < 预设距离时，让机械臂闭合 | **5. 搬运归位**  **寻找归位区** | 机器人携带箱子，**IMX577** 寻找对应的地面色块。 | **YOLOv8** 或 **OpenCV HSV** | 1. 根据手里抓的箱子类型 (如食品-绿色)，检索对应的归位区 (绿色方块)。<br>2. 识别地面上的绿色矩形区域。<br>3. 计算矩形中心与机器人的相对角度。
| **归位** | 前往归位区，**L2 雷达** 全局定位 | **几何计算** | 直接导航至对应颜色（这个是否可以预设？）的归位区 |
| **放置物资** | TOF 测距 + IMX577 边框检测 | **CV计算** | 防压线：视觉识别归位区 (400mm x 400mm) 的边框（具体还是要看机械臂怎么实现抓取）|

### 时间规划：
12月：YOLO训练集 + 仿真环境搭建 + 算题识别
1月（期末月）：硬件选型敲定 + 识别仿真（这个暂时比较少，怕考试来不及做，如果时间充裕可以往前挪）
2月：SLAM方案 + 制订拾取方案决策树 + 机械臂
3月：完成完整拾取流程 + 完成全场定位并仿真测试
4月：实测
5月：调整
6月（期末月）：